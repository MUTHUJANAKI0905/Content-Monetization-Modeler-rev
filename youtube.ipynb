pip install pandas
pip install matplotlib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Load the dataset from the specified path
df = pd.read_csv('youtube_ad_revenue_dataset.csv')

# Display the first few rows to get a quick overview
print("Initial Data:")
print(df.head())

# Get a summary of the DataFrame including data types and non-null counts
print("\nDataFrame Info:")
df.info()
pip install seaborn
import seaborn as sns

# Plot the distribution of the target variable, 'ad_revenue_usd'
plt.figure(figsize=(10, 6))
sns.histplot(df['ad_revenue_usd'], bins=50, kde=True)
plt.title('Distribution of Ad Revenue')
plt.xlabel('Ad Revenue (USD)')
plt.ylabel('Frequency')
plt.show()

# Visualize the correlation matrix of numerical features
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns
plt.figure(figsize=(12, 8))
sns.heatmap(df[numerical_cols].corr(), annot=True, cmap='viridis', fmt='.2f')
plt.title('Correlation Matrix')
plt.show()
pip install scikit-learn
import numpy as np
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split

# Handle missing values using imputation (median for numerical, mode for categorical)
for col in df.columns:
    if df[col].dtype in ['int64', 'float64']:
        df[col].fillna(df[col].median(), inplace=True)
    else:
        df[col].fillna(df[col].mode()[0], inplace=True)

# Remove duplicate records
df.drop_duplicates(inplace=True)

# Create a new feature: engagement rate
df['engagement_rate'] = (df['likes'] + df['comments']) / df['views']
df['engagement_rate'].replace([np.inf, -np.inf], 0, inplace=True)
df['engagement_rate'].fillna(0, inplace=True)

# Define features (X) and target (y)
X = df.drop(['video_id', 'date', 'ad_revenue_usd'], axis=1)
y = df['ad_revenue_usd']

# One-hot encode categorical features
categorical_cols = X.select_dtypes(include=['object']).columns
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Scale numerical features for better model performance
numerical_cols = X_encoded.select_dtypes(include=['int64', 'float64']).columns
scaler = StandardScaler()
X_encoded[numerical_cols] = scaler.fit_transform(X_encoded[numerical_cols])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Initialize a dictionary of models
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    results[name] = {'R² Score': r2, 'RMSE': rmse, 'MAE': mae}

# Print the results
for name, metrics in results.items():
    print(f"--- {name} ---")
    print(f"R² Score: {metrics['R² Score']:.4f}")
    print(f"RMSE: {metrics['RMSE']:.2f}")
    print(f"MAE: {metrics['MAE']:.2f}\n")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor

# Step 1: Create a sample dataset (replace this with your actual data)
# 'features' are the columns that influence revenue
# 'revenue' is the target variable you are trying to predict
data = {
    'Ad_Spend': np.random.rand(100) * 1000,
    'Website_Traffic': np.random.randint(500, 5000, 100),
    'Customer_Reviews': np.random.randint(1, 5, 100),
    'Email_Campaign_Clicks': np.random.randint(10, 200, 100)
}
df = pd.DataFrame(data)

# Create a 'revenue' column that is a function of the features with some noise
df['Revenue'] = (
    df['Ad_Spend'] * 5.0 + 
    df['Website_Traffic'] * 0.2 + 
    df['Email_Campaign_Clicks'] * 1.5 + 
    np.random.randn(100) * 1000
)

# Step 2: Separate features (X) and target (y)
X = df.drop('Revenue', axis=1)
y = df['Revenue']

# Step 3: Train the model
# We'll use a Random Forest Regressor as it has a built-in feature_importances_ attribute
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X, y)

# Step 4: Get and organize feature importances
# The model.feature_importances_ attribute contains the importance scores
importances = model.feature_importances_
feature_names = X.columns

# Create a DataFrame to store the results
feature_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
})

# Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

print("Feature Importances:")
print(feature_importance_df)

# Step 5: Visualize the results
plt.figure(figsize=(10, 6))
sns.barplot(x='importance', y='feature', data=feature_importance_df)
plt.title('Feature Importance for Revenue Prediction Model')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()
